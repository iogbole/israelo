---
layout: post
title:  "Unlocking Free Performance Gains with Profile-Guided Optimization (PGO): A Practical Implementation and Benchmark Analysis"
author: israel
categories: [ 'Cloud Native' ]
tags: [containers, devops, cloud-native, kubernetes, ebpf ]
image: https://github.com/user-attachments/assets/de1c459a-f58c-427b-b187-89ef2593b9c7
date:   2024-07-26 06:01:35 +0300
description: "PGO can significantly improve code performance and resource utilization, potentially saving up to 14% of cloud spending without requiring any code changes." 

---


**Key Takeaways:**

-   PGO can significantly improve code performance and resource utilization, potentially saving up to 14% of cloud spending without requiring any code changes.
-   PGO is a valuable optimization technique for various programming languages, including C/C++, Go, Rust, Java, .NET, Swift, Python, and Fortran.
-   PGO utilizes profiling data to guide the compiler in making more informed optimization decisions, tailoring the code to its actual usage patterns.
-   Many developers and SREs are missing out on potential cost savings by not using PGO.

### What is Profile-Guided Optimization (PGO)?

Imagine you're an athlete training for the Olympics. A generic training regimen might get you started, but it won't fully optimize your performance. A personal coach observes your training, identifies areas for improvement, and adjusts your regimen accordingly. This personalized approach leads better results in a shorter time.

PGO is like that coach for your code. It analyzes your code's performance, identifies frequently executed code paths, and feeds that data to the compiler to refine its optimization decisions. By informing the compiler of your code's hotspots, PGO helps your code achieve peak performance, just as a coach helps an athlete get an Olympic gold meal. 

To connect the dots, the method by which the coach observes the athlete's training regimen and physiological component is called `profiling` in this context. Just as a coach **scouts out**  their candidates, **continuous profiling** analyzes your code's behavior on a continous basis.

PGO is an advanced optimization technique that leverages runtime profiling data to guide the compiler in making more informed optimization decisions. By using PGO, the compiler can optimize hot code paths, potentially leading to significant performance improvements.

PGOs can improve your code's resouce utilization efficiency by upto `14%`. That's alot of cost savings in most medium - to large organisations without making any code changes, that's free money on table!  


> As of Go 1.22, benchmarks for a representative set of Go programs show
> that building with PGO improves performance by around 2-14%. We expect
> performance gains to generally increase over time as additional
> optimizations take advantage of PGO in future versions of Go. (source:

 *[PGO Overview](https://go.dev/doc/pgo#overview), Google)*


Cloudflare recently shared a [blog post](https://blog.cloudflare.com/reclaiming-cpu-for-free-with-pgo) detailing how they significantly reduced cloud spending by implementing Profile-Guided Optimization (PGO).

> This indicates that following the release, we’re using ~97 cores fewer
> than before the release, a ~3.5% reduction. 
> 
*[Colin Douch](https://blog.cloudflare.com/author/colin),  Cloudflare.* 

### Compilers and the One-Size-Fits-All Problem
As described in the athlete analogy above ( inspired by the recent 2024 Paris Olympics), one of the compiler's primary tasks is to make optimal decisions about your code during compilation. Compilers come equipped with heuristics that guide various optimization techniques—such as dead code elimination, register allocation, constant folding, and function inlining. These strategies are designed to streamline your code and enhance its efficiency.

However, the heuristics that guide the compiler's decisions have the same problem inherent in most one-size-fits-all designs. There are limits to what a compiler can do on its own. For example, while inlining functions can reduce the overhead of function calls, the compiler can't inline everything. Inlining too much can lead to bloated binaries, increased cache usage, and ultimately, performance degradation. 

This is where PGO comes in. By providing the compiler with profiling data—information about how your code actually runs in real-world scenarios—the compiler can make more informed decisions. It can identify the "hot" functions that are frequently called and optimize them more aggressively, such as by inlining these critical functions or better allocating registers. This targeted optimization helps reduce the overhead of function calls and can lead to significant performance gains. PGO allows the compiler to go beyond generic optimizations, tailoring the final binary to the specific needs of your program based on actual usage.

### How does PGO work? 

Several other languages support PGO. See  [Laguages that support PGO](#laguages_that_support_pgo) section.  In this example, let's consider a simple Go application that unmarshals JSON data containing personal bios. The application reads data from a file, processes it, and then serves it over HTTP.  Here's the codehttps://github.com/iogbole/go-pgo 


#### **Step 1: Compile and Run Without PGO**

First, let's compile the program **without PGO** and see how it performs under load:

```sh

# Compile the code without PGO
go build -o pre_pgo -gcflags="-m" main.go

# Run
./pre_pgo

```
#### **Step 2: Load Testing with gwrk**

To simulate real-world usage, we can use `gwrk`, a Go-based benchmarking tool, to send concurrent HTTP requests to the service.  Install it from [here](https://github.com/tsliwowicz/go-wrk). 

 ```sh

 go-wrk -d 20 http://localhost:8080
 
 ``` 
This command runs the load test for 60 seconds, generating HTTP requests to the server. The results will give us a baseline of how the program performs without PGO.

#### **Step 3: Generate Profiling Data**

While that's running, let's profile the code for 30 seconds. 

```sh

curl -o default.pgo "http://localhost:8080/debug/pprof/profile?seconds=30"

```

Note that `default.pgo` is the `pprof` output. An alternative approach is to use the pprof tool. 

```sh
go tool pprof -output=cpu.prof http://localhost:8080/debug/pprof/profile?seconds=60

```

#### **Step 4: Compile and Run post PGO**

Now, let's compile the code with PGO enabled using the profiling data we collected:

```sh

 # Compile the program with PGO
 go build -o post_pgo -pgo=auto -gcflags="-m" main.go 
 # Run the optimized program 
 ./post_pgo

```

#### **Step 5: Add load and Profile **
Similar to step 3, gnerate load and profile - but this time, change the output of the curl to `post_pgo.pprof`.

```cpp

curl -o post_pgo.pprof  "http://localhost:8080/debug/pprof/profile?seconds=30" 

```

### The Results!!

This was my first experience with PGO, and I was eager to see if it was worth the effort. I've saved all the raw benchmark files in the GitHub [repository](https://github.com/iogbole/go-pgo) for future comparisons. Let's begin!

#### **Is Inlining Better? **

Inlining is a compiler optimization technique that directly inserts the body of a function into the calling code, eliminating function call overhead and potentially enabling further optimizations.**

A quick way to check for better inlining is to examine the binary size. While inlining can improve performance, it may increase the binary size due to code duplication.

Let's check it out:

```sh

$ ls -atrl
-rwxr-xr-x   1 israel  staff  8162050 Aug 23 18:45 pre_pgo
-rwxr-xr-x   1 israel  staff  8244658 Aug 23 21:52 post_pgo

```

The difference in file sizes between the `pre_pgo` and `post_pgo` binaries can be summarized as follows:

| **File**        | **Size**    | **Difference**               | **Analysis**                                                                 |
|-----------------|-------------|------------------------------|-------------------------------------------------------------------------------|
| **`pre_pgo`**   | 8,162,050 bytes |                              | This is the size of the binary before applying Profile-Guided Optimization (PGO). |
| **`post_pgo`**  | 8,244,658 bytes | **82,608 bytes larger** | The binary size increased slightly after PGO, likely due to additional inlining and optimizations. |

#### **Analysis of the compiler output? **
 Let's dig a bit deeper by comparing the build outputs: 


```cpp
go build -o without_pgo -gcflags="-m" main.go

# command-line-arguments
./main.go:44:6: can inline extractNames
./main.go:75:14: inlining call to fmt.Println
./main.go:76:14: inlining call to fmt.Println
./main.go:77:14: inlining call to fmt.Println
[Truncated] 
```
...

```cpp
go build -o with_pgo -pgo=auto -gcflags="-m" main.go
# command-line-arguments
./main.go:109:9: PGO devirtualizing interface call w.Write to http.(*response).Write
./main.go:51:6: can inline extractNames
./main.go:67:6: can inline processor
./main.go:89:13: inlining call to fmt.Println
[Truncated] 

```
Refer to the full compiler outputs in the repo, [here](https://github.com/iogbole/go-pgo/blob/main/compilation_outpts.txt). 

Here's a table comparing the key differences between the compiler output with and without PGO, based on the provided `gcflags="-m"` output:

| **Metric**                                   | **post PGO**                               | **pre PGO**                                  | **Difference**                                          | **Analysis**                                               |
|----------------------------------------------|------------------------------------------------|------------------------------------------------|---------------------------------------------------------|-------------------------------------------------------------|
| **Inlining**                                 | `extractNames`, `fmt.Println`, `http.ListenAndServe`, etc. | `extractNames`, `processor`, `http.(*response).Write`, etc. | More functions inlined, including `processor` and HTTP response writes | More extensive inlining in PGO due to better optimization hints |
| **Parameter Leaks**                          | Multiple parameters escape to the heap | Similar, with some optimizations such as better handling in `extractNames` | Slight improvement in handling heap allocation in PGO | PGO provides better guidance for optimizing heap allocations |
| **Moved to Heap**                            | Variables like `bios` moved to heap            | Variables like `bioWrapper` moved to heap      | Slight change in what gets moved to heap                | Heap allocations are slightly optimized with PGO, but not eliminated |
| **PGO Devirtualization**                     | Not applicable                                 | PGO devirtualizes interface calls              | Devirtualization is applied                              | PGO enables devirtualization, improving performance by optimizing interface calls |
| **Leaking Param**                            | `data`, `w`, etc. escape to heap              | Similar parameters leak but with better optimizations such as `extractNames` | Slight reduction in the number of heap escapes          | PGO guides more efficient memory management, but some leaks persist |


#### Flamegrapsh Analysis   

Using [https://www.speedscope.app](https://www.speedscope.app), I uploaded the before and after pprof files for quick analysis. Since Speedscope lacks a differential flamegraph feature, I compared both flamegraphs side-by-side.

<img width="2600" alt="speedscope" src="https://github.com/user-attachments/assets/c362a17c-ee13-423d-b1cf-22b455e9531c">

The table below summarises analysis of the before and after PGO flmaegraph analysis. 

| **Metric**                               | **Pre PGO (Left)** | **Post PGO (Right)** | **Time Gained/Lost**            | **Percentage Improvement**                  |
|------------------------------------------|-----------------------|-----------------------|----------------------------------|---------------------------------------------|
| **`main.processor` Total Time**          | 48.25 seconds         | 44.91 seconds         | **3.34 seconds gained**          | **6.92% improvement**                        |
| **`main.processor` Self Time**           | 10.00ms               | 0.00ns                | **10.00ms gained**               | **100% improvement**                        |
| **HTTP Handler Functions Total Time**    | Higher                | Lower                 | Not Quantified                   | Slight improvement (qualitative)             |
| **Syscall Execution Time**               | Higher                | Lower                 | Not Quantified                   | Slight improvement (qualitative)             |


### Laguages that support PGO 

PGO is supported by several popular programming languages and their respective compilers. Here are some of the key languages that support PGO:

1.  **C/C++**:
    
    -   **GCC (GNU Compiler Collection)**: Supports PGO through options like `-fprofile-generate` and `-fprofile-use`.
    -   **Clang/LLVM**: Also supports PGO with similar flags like `-fprofile-generate` and `-fprofile-use`.
    -   **Microsoft Visual C++ (MSVC)**: Supports PGO through `/LTCG:PGInstrument`, `/LTCG:PGOptimize`, and related flags.
2.  **Go**:
    
    -   **Go**: Starting from Go 1.20, the Go compiler supports PGO. Profiles are collected using the Go `pprof` tool and applied during compilation.
3.  **Rust**:
    
    -   **Rust**: Supports PGO via `rustc` with flags like `-Cprofile-generate` and `-Cprofile-use`.
4.  **Java**:
    
    -   **Oracle HotSpot JVM**: Java applications can benefit from PGO via the HotSpot JVM, which uses profiling data to optimize JIT (Just-In-Time) compilation.
    -   **GraalVM**: GraalVM also supports PGO for optimizing Java applications.
5.  **.NET (C# and other .NET languages)**:
    
    -   **.NET Runtime**: The .NET Just-In-Time (JIT) compiler can use PGO to optimize code paths based on runtime profiling data.
6.  **Swift**:
    
    -   **Swift**: The Swift compiler supports PGO, leveraging LLVM’s PGO infrastructure.
7.  **Python**:
    
    -   **CPython**: Python can be compiled with PGO by running `./configure --enable-optimizations`, which uses profile data to optimize the interpreter itself.
8.  **Fortran**:
    
    -   **GFortran**: As part of GCC, GFortran supports PGO using the same flags as C/C++.

These languages leverage PGO to optimize performance by using runtime profiling data to guide the compiler in making more informed optimization decisions, tailoring the final binary to better suit the actual usage patterns of the application.
 
### Conculsion 

This blog post explored Profile-Guided Optimization (PGO) and its potential benefits for improving code performance.

We used a simple Go application as an example, demonstrating the steps involved in applying PGO:

1.  **Compile and Run Without PGO**: Establish a baseline performance metric.
2.  **Load Testing**: Simulate real-world usage with tools like `gwrk`.
3.  **Generate Profiling Data**: Capture runtime behavior using `pprof`.
4.  **Compile and Run with PGO**: Leverage profiling data for optimizations.
5.  **Analyze Results**: Compare performance improvements and assess optimizations through techniques like flamegraph analysis.

The example showcased potential performance gains through PGO, including reduced execution time and optimized memory management. It's important to note that specific results may vary depending on the application and profiling data.

**Why aren't more developers leveraging the power of PGO?** Share your thoughts and experiences in the comments below.


